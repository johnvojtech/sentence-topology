% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@inproceedings{agirre2015semeval,
  title={Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability},
  author={Agirre, Eneko and Banea, Carmen and Cardie, Claire and Cer, Daniel and Diab, Mona and Gonzalez-Agirre, Aitor and Guo, Weiwei and Lopez-Gazpio, Inigo and Maritxalar, Montse and Mihalcea, Rada and others},
  booktitle={Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015)},
  pages={252--263},
  year={2015}
}
@ARTICLE{9140343,
  author={Wang, Bin and Kuo, C.-C. Jay},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={SBERT-WK: A Sentence Embedding Method by Dissecting BERT-Based Word Models}, 
  year={2020},
  volume={28},
  number={},
  pages={2146-2157},
  doi={10.1109/TASLP.2020.3008390}}
@inproceedings{opitz-frank-2022-sbert,
    title = "{SBERT} studies Meaning Representations: Decomposing Sentence Embeddings into Explainable Semantic Features",
    author = "Opitz, Juri  and
      Frank, Anette",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-main.48",
    pages = "625--638",
   
}
@article{cutler2012random,
  title={Random forests},
  author={Cutler, Adele and Cutler, D Richard and Stevens, John R},
  journal={Ensemble machine learning: Methods and applications},
  pages={157--175},
  year={2012},
  publisher={Springer}
}
@article{calinski1974dendrite,
  title={A dendrite method for cluster analysis},
  author={Cali{\'n}ski, Tadeusz and Harabasz, Jerzy},
  journal={Communications in Statistics-theory and Methods},
  volume={3},
  number={1},
  pages={1--27},
  year={1974},
  publisher={Taylor \& Francis}
}

@book{scholkopf1999advances,
  title={Advances in kernel methods: support vector learning},
  author={Sch{\"o}lkopf, Bernhard and Burges, Christopher JC and Smola, Alexander J and others},
  year={1999},
  publisher={MIT press}
}
@article{smola2004tutorial,
  title={A tutorial on support vector regression},
  author={Smola, Alex J and Sch{\"o}lkopf, Bernhard},
  journal={Statistics and computing},
  volume={14},
  pages={199--222},
  year={2004},
  publisher={Springer}
}
@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{lau-baldwin-2016-empirical,
    title = "An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation",
    author = "Lau, Jey Han  and
      Baldwin, Timothy",
    booktitle = "Proceedings of the 1st Workshop on Representation Learning for {NLP}",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-1609",
    doi = "10.18653/v1/W16-1609",
    pages = "78--86",
}

@ARTICLE{5298967,
  author={Singh, Ravindra and Pal, Bikash C. and Jabr, Rabih A.},
  journal={IEEE Transactions on Power Systems}, 
  title={Statistical Representation of Distribution System Loads Using Gaussian Mixture Model}, 
  year={2010},
  volume={25},
  number={1},
  pages={29-37},
  doi={10.1109/TPWRS.2009.2030271}}

@article{reynolds2009gaussian,
  title={Gaussian mixture models.},
  author={Reynolds, Douglas A and others},
  journal={Encyclopedia of biometrics},
  volume={741},
  number={659-663},
  year={2009},
  publisher={Berlin, Springer}
}
@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}
@inproceedings{baranvcikova2020costra,
  title={Costra 1.1: An Inquiry into Geometric Properties of Sentence Spaces},
  author={Baran{\v{c}}{\'\i}kov{\'a}, Petra and Bojar, Ond{\v{r}}ej},
  booktitle={Text, Speech, and Dialogue: 23rd International Conference, TSD 2020, Brno, Czech Republic, September 8--11, 2020, Proceedings},
  pages={135--143},
  year={2020},
  organization={Springer}
}


@inproceedings{le2014distributed,
  title={Distributed representations of sentences and documents},
  author={Le, Quoc and Mikolov, Tomas},
  booktitle={International conference on machine learning},
  pages={1188--1196},
  year={2014},
  organization={PMLR}
}